{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:莊喻捷\n",
    "\n",
    "Student ID:107061538\n",
    "\n",
    "GitHub ID:chuangyc\n",
    "\n",
    "Kaggle name:jack3168\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM19-Lab2-Master Repo](https://github.com/EvaArevalo/DM19-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/179d01d4dd984fc5ac45a894822479dd) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 23rd 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/EvaArevalo/DM19-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM19-Lab2-Homework](https://github.com/EvaArevalo/DM19-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "### training data\n",
    "anger_train = pd.read_csv(\"data/semeval/train/anger-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_train = pd.read_csv(\"data/semeval/train/sadness-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_train = pd.read_csv(\"data/semeval/train/fear-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_train = pd.read_csv(\"data/semeval/train/joy-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine 4 sub-dataset\n",
    "train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text emotion  intensity\n",
       "0  10000  How the fu*k! Who the heck! moved my fridge!.....   anger      0.938\n",
       "1  10001  So my Indian Uber driver just called someone t...   anger      0.896\n",
       "2  10002  @DPD_UK I asked for my parcel to be delivered ...   anger      0.896\n",
       "3  10003  so ef whichever butt wipe pulled the fire alar...   anger      0.896\n",
       "4  10004  Don't join @BTCare they put the phone down on ...   anger      0.896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing data\n",
    "anger_test = pd.read_csv(\"data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_test = pd.read_csv(\"data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_test = pd.read_csv(\"data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_test = pd.read_csv(\"data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "\n",
    "# combine 4 sub-dataset\n",
    "test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     1585\n",
       "to      1273\n",
       "a       1145\n",
       "I       1042\n",
       "and      912\n",
       "of       778\n",
       "is       757\n",
       "in       588\n",
       "you      567\n",
       "my       453\n",
       "for      431\n",
       "that     419\n",
       "on       362\n",
       "it       359\n",
       "be       340\n",
       "me       304\n",
       "have     290\n",
       "so       279\n",
       "this     275\n",
       "with     272\n",
       "not      263\n",
       "at       249\n",
       "but      242\n",
       "I'm      238\n",
       "just     238\n",
       "was      219\n",
       "like     216\n",
       "are      213\n",
       "your     209\n",
       "all      198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "train_df.text.str.split(expand=True).stack().value_counts()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheerful',\n",
       " 'cheering',\n",
       " 'cheery',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'country',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'customer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "\n",
    "BOW_500 = TfidfVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape\n",
    "train_data_BOW_features_500.toarray()\n",
    "feature_names_500 = BOW_500.get_feature_names()\n",
    "feature_names_500[100:110]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer here\n",
    "The result means that \n",
    "1. it can predict the fear label most correct.\n",
    "2. It is likely to view fear as anger\n",
    "3. It is likely to view sadness as fear\n",
    "4. Anger and joy will not be mismatch comparing to other pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? \n",
    "\n",
    "*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fear', 'anger', 'anger', 'anger', 'sadness', 'anger', 'anger',\n",
       "       'anger', 'anger', 'anger'], dtype='<U7')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "## build DecisionTree model\n",
    "#DT_model = DecisionTreeClassifier(random_state=0)\n",
    "clf = MultinomialNB()\n",
    "## training!\n",
    "#DT_model = DT_model.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "## predict!\n",
    "\n",
    "y_train_pred_NB = clf.predict(X_train)\n",
    "y_test_pred_NB = clf.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred_NB[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XecFeXZxvHfxaJSBSkqVizYFeyC\nvTesmKAxCWKPGjUaS6JRo7FEfTXWKLFh19hbbKhYUUFFsMVo1CCK0jtS7vePmTVHs+wup+zs7F5f\nP+ez58zMPnMP53ifZ+955hlFBGZmlo0WWQdgZtacOQmbmWXISdjMLENOwmZmGXISNjPLkJOwmVmG\nnITNzDLkJGxmliEnYTOzDLXMOgAzs2K1b90p5i+YW1Ibs76b/lRE7Law9ZJuAvoC30TEeumyTsA9\nQHfgM+CnETFJkoArgD2AmcAhEfFWbft3Ejaz3Jq/YC6rL7tRSW2M+uLFLnVscgtwNXBrwbLTgSER\ncZGk09PXpwG7Az3Sx+bAX9OfC+VyhJnlmqSSHnWJiBeBiT9avA8wOH0+GNi3YPmtkRgGdJTUrbb2\nnYTNrLnrIml4wePIevzOMhHxFUD6c+l0+fLAfwq2G5MuWyiXI8wsx4RUcl9yfERsUo5ogJq61rVO\nVemesJnZohtXXWZIf36TLh8DrFiw3QrA2NoachI2s1xrgUp6FOkRYED6fADwcMHyXyqxBTClumyx\nMC5HmJnVQtJdwHYkteMxwNnARcC9kg4DvgB+km7+BMnwtH+RDFEbWFf7TsJmlluCeo1wKEVEHLSQ\nVTvWsG0Axy5K+y5HmJllyD1hM8u1FqWPjsiUk7CZ5Vc9L7hozPL9FWJmlnPuCZtZrqn4YWaNgnvC\nZmYZck/YzHJL5P/EXL6jNzPLOfeEzSzXPDrCzMyK5p6wmeWYaOGesJmZFcs9YTPLLQHKeV/SSdjM\ncs0n5szMrGjuCZtZfgmfmDMzs+K5J2xmOSZP4GNmZsVzT9jMcssT+JiZWUncEzazXMv7OGEnYTPL\nMc8dYWZmJXASNjPLkMsRZpZbyQQ++S5HOAmbWa7lfYiak7CZ5ZfyPzoi318hZmY55yRsFSWptaRH\nJU2R9PcS2jlY0tPljC0rkraW9FHWcVjj4CRsAEj6maThkqZL+krSPyRtVYamDwCWATpHxE+KbSQi\n7oiIXcoQT0VJCkmr17ZNRLwUEWs2VExNmdJxwqU8suYkbEg6CfgLcAFJwlwJuBbYpwzNrwz8MyLm\nlaGt3JPk8zBlphL/y5qTcDMnqQNwLnBsRDwQETMiYm5EPBoRp6TbLCHpL5LGpo+/SFoiXbedpDGS\nTpb0TdqLHpiu+yNwFtA/7WEfJukcSbcX7L972ntsmb4+RNKnkqZJ+rekgwuWv1zwe30kvZmWOd6U\n1Kdg3QuSzpP0StrO05K6LOT4q+M/tSD+fSXtIemfkiZK+n3B9ptJek3S5HTbqyUtnq57Md1sZHq8\n/QvaP03S18DN1cvS31kt3cdG6evlJI2XtF1Jb6zlhpOw9QZaAQ/Wss0ZwBZAL6AnsBlwZsH6ZYEO\nwPLAYcA1kpaKiLNJetf3RES7iLixtkAktQWuBHaPiPZAH+CdGrbrBDyebtsZuAx4XFLngs1+BgwE\nlgYWB35by66XJfk3WJ7kS+NvwM+BjYGtgbMkrZpuOx/4DdCF5N9uR+AYgIjYJt2mZ3q89xS034nk\nr4IjC3ccEZ8ApwF3SGoD3AzcEhEv1BKvNSFOwtYZGF9HueBg4NyI+CYivgX+CPyiYP3cdP3ciHgC\nmA4UW/NcAKwnqXVEfBUR79WwzZ7AxxFxW0TMi4i7gA+BvQq2uTki/hkRs4B7Sb5AFmYucH5EzAXu\nJkmwV0TEtHT/7wEbAETEiIgYlu73M+B6YNt6HNPZETEnjecHIuJvwMfA60A3ki89qydJJT2y5iRs\nE4AuddQqlwM+L3j9ebrs+zZ+lMRnAu0WNZCImAH0B44GvpL0uKS16hFPdUzLF7z+ehHimRAR89Pn\n1UlyXMH6WdW/L2kNSY9J+lrSVJKefo2ljgLfRsTsOrb5G7AecFVEzKljWyvgE3OWd68Bs4F9a9lm\nLMmf0tVWSpcVYwbQpuD1soUrI+KpiNiZpEf4IUlyqiue6pi+LDKmRfFXkrh6RMSSwO+hzrM7UdtK\nSe1ITozeCJyTllusmXASbuYiYgpJHfSa9IRUG0mLSdpd0sXpZncBZ0rqmp7gOgu4fWFt1uEdYBtJ\nK6UnBX9XvULSMpL2TmvDc0jKGvNraOMJYI10WF1LSf2BdYDHioxpUbQHpgLT0176r360fhyw6v/8\nVu2uAEZExOEkte7rSo7ScsNJ2IiIy4CTSE62fQv8BzgOeCjd5E/AcOBdYBTwVrqsmH09A9yTtjWC\nHybOFsDJJD3diSS11mNqaGMC0DfddgJwKtA3IsYXE9Mi+i3JSb9pJL30e360/hxgcDp64qd1NSZp\nH2A3khIMJO/DRtWjQqx21RP45HmImiJq/UvJzKzRWqpNl9hhrb4ltfHA24NHRMQmZQppkXnguJnl\nlyfwMTOzUjgJm5llyOUIM8stQaMY61sKJ2Ezy7XGMMKhFE7CNViyVdvo2r5pjpdv26FV1iFUVIvF\nm+5HOhY03ZFMY77+mgmTJ+c7mxap6X5iS9C1fScu2P+krMOoiM13q3Wq29xrv/LSWYdQMfNmNd2r\nmXc57Mi6N2qinITNLMcax/wPpXASNrPcEvkfJ+wkbGa5lveesMcJm5llyEnYzCxDLkeYWa55nLCZ\nWUYk14TNzKwETsJmZhlyOcLMcqxx3DG5FE7CZpZrrgmbmTVhkn4j6T1JoyXdJamVpFUkvS7pY0n3\nSFq82PadhM3MFkLS8sDxwCYRsR5QBRwI/Bm4PCJ6AJOAw4rdh5OwmeVaA9xtuSXQWlJLoA3wFbAD\ncF+6fjCwb7HxuyZsZrlVpjtrdJE0vOD1oIgYBBARX0q6FPgCmAU8DYwAJkfEvHT7McDyxe7cSdjM\ncq0MoyPGL+yW95KWAvYBVgEmA38Hdq9h06Jn3Hc5wsxs4XYC/h0R30bEXOABoA/QMS1PAKwAjC12\nB07CZmYL9wWwhaQ2SrrcOwLvA88DB6TbDAAeLnYHLkc0sL7nH8rcOXOJBQuIBcEzF9xJz35bs9wG\nq7Jg3nymfzuFNwY/zdwc3srm1Kuv4Lnhw+ncoQNPXXE1AO//+1POuO5a5sydS8uqKs498mh69Vgj\n40hLt2HffWnXpi1VVS2oqqpiyO23ZB1S2fzt3vu4/dHHiAh+vndfjvzpT7IOaeFU2TtrRMTrku4D\n3gLmAW8Dg4DHgbsl/SlddmOx+3ASzsDz//d3vpsx+/vXX7//Oe8++DKxINhg/61Ye/dNefeBlzOM\nsDj9tt+RX+7el5OvvPz7ZRfeegsn9D+I7TbamOdHDOeiW2/h7vMuyDDK8nno+mvovFTHrMMoqw8+\n/ZTbH32Mf/ztOhZv2ZKDTj6VnXr3ZtUVV8g6tBo1xJ01IuJs4OwfLf4U2Kwc7bsc0QiM++CL7++k\nO+HTr2jTsV3GERVn83XXo2P7H8YuiekzZwIwbeYMlunUNO9i3VR8/NnnbLzuOrRp1YqWLVvSe8Oe\nPPHii1mH1aS5J9zAAtjuxP2JgE9eGsWnL436wfpVtlyP/wz/KJvgKuCsQw9nwLlnc8Hgm1kQC7jv\ngouzDqksJHHAsccjiQH99mPA/kUPE21U1lp1FS4adAMTp0yh1RJLMOS1YfRca82sw2rSnIQb2JCL\n72H2lBks0b41253Qj2lfT+Tbj78EYO3dNyPmL+Dz1z/MOMryuf3Jf3DmwMPZvXcfHnvlZU6/9ipu\nP+e8rMMq2eM3DaJb1658O3EiBxxzPD26r0yfjTbMOqySrdG9O8f9/Gf0/83JtG3dmnVXX52WVY07\nTeR9UvdmV45QIrPjnj1lBgBzps1izDv/olP3ZQHovsU6LLfBKgy78R9ZhVYRD7zwHLtt0RuAPfts\nyciP/5lxROXRrWtXALp26sQe22/LW6Pfzzii8vlZ3z155qYbeOiaq+i4ZHtWWaHo6xAaRAuV9sha\no0nCkh6SNCKdKOPIdNl0SedLGilpmKRl0uWrpa/flHSupOkF7ZySLn9X0h/TZd0lfSDpWpKznCtm\ncYxVi7ek5RKLff982XVWZsrY8Sy77sqstesmvHzNI8yfO6+OVvJl6aU68fp7owF4ddS7dO+2XMYR\nlW7GrFlMmzHj++cvDHuDtVdfNeOoyufbSZMAGPP1OJ4Y+hL77bRTxhE1bY3p74xDI2KipNbAm5Lu\nB9oCwyLiDEkXA0cAfwKuAK6IiLskHV3dgKRdgB4kZy0FPCJpG5KxfmsCAyPimJp2nib+IwG6tFuq\nIgfYasm2bHX0Xsn+qlrw+Rsf8vV7n7PHeQOpalnFtifuD8CET79mxJ1DKhJDJR1/2SUMGz2aSdOm\n0vvwgZx44EFceMxxnHvj35g3fz5LLL44F/zq2KzDLNm3EyYy4LenATBv/nz67bYLO/bpnXFU5XP4\nGX9g4tSpLFbVkgtPOpGOS7bPOqRa5X0+YUUUfbVdWUk6B9gvfdkd2BUYCrSKiJDUH9g5Ig6XNAFY\nJiLmSVoSGBsR7dJrvA8gubwQoB1wITAEeD4iVqlPLKt1XTEu2P+kch1ao7L5bqtnHUJFtV956axD\nqJh5ORw7Xl+7HHYkIz/8cJGz6dLtl4n+G/+spH1fPfQvIxZ22XJDaBQ9YUnbkVwe2DsiZkp6AWgF\nzI3/fkvMp+54BVwYEdf/qP3uwIwyhmxmjUCZJvDJVGOpCXcAJqUJeC1gizq2Hwb0S58fWLD8KeBQ\nSe0gmQtUUtPtGpk1d0pub1TKI2uNJQk/CbSU9C5wHkmSrc2JwEmS3gC6AVMAIuJp4E7gNUmjSOb7\nbNwFLTNr1hpFOSIi5lDz9HDtCra5j/9OovwlsEVaKz4QGF6w3RUkJ+5+bL3yRWxmjUWLnI8TbhRJ\nuAgbA1ensxpNBg7NOB4zs6LkMglHxEtAz6zjMLNsNcQEPpXWWGrCZmbNUi57wmZm1TxEzczMiuae\nsJnlWs47wu4Jm5llyT1hM8utpnDZspOwmeWYPKm7mZkVzz1hM8sv+WINMzMrgXvCZpZreT8x556w\nmVmG3BM2s9xKJvDJOorSuCdsZpYh94TNLNfyXhN2EjazXPPFGmZmVjT3hM0st4RyX45wT9jMLEPu\nCdeg/dLt2fnYrbMOoyK23eukrEOoqKGPXpZ1CBWzWPs2WYdQMaoqsj8oD1EzM7MSuCdsZrnmCXzM\nzKxo7gmbWa7lfXSEk7CZ5ZbnjjAzs5I4CZuZZcjlCDPLNdeEzcwy5Al8zMysaO4Jm1l+Sb5Yw8zM\niueesJnlloAW+e4IOwmbWb65HGFmZkVzEjYzy5DLEWaWa3kvRzgJm1luSfk/MedyhJlZhpyEzcwy\ntNByhKRRQNS0CoiI2KBiUTUDs+fMoe/AI/lu7lzmzZvH3jvvyOnHHJV1WIvs3EtOY5sd+jBxwiT2\n3+UQAJbs0J5LrzmH5VboxtgxX/HbY85m6tTprLLaSpx36emsve4aXHnpDQwedHe2wRepqbx3Ncnj\nsTXlmnDfBouiGVpi8cV56Ia/0q5NG+bOnccehxzOjlv1YdMN1s86tEXy8N+f5K7BD3L+Zb//ftlh\nxxzM66+8xY1/vYPDfnUwhx3zcy6/6DqmTJ7KhWdfyQ67bpVhxKVrKu9dTfJ4bDnPwQsvR0TE59WP\ndFGP9Pk3wMQGia4Jk0S7NsktzOfOm8e8efNyORvUiDdGMmXy1B8s237nrXj4/icBePj+J9l+lyTp\nTpwwmffe/ZB5c+c3eJzl1FTeu5rk8dhaSCU9slZnTVjSEcB9wPXpohWAhyoZVHMxf/58tv3pz1hr\n+13YdovN2WSD9bIOqSw6d1mK8d9MAGD8NxPo3GWpjCMqv6b63kHTPrbGqD4n5o4FtgSmAkTEx8DS\nlQxqYSQdL+kDSXdksf9yq6qqYui9dzLq6cd5e/R7fPDxv7IOyeqpKb93TfnYGqP6JOE5EfFd9QtJ\nLan5hF1DOAbYIyIOLrYBSVVljKcsOizZni033Zghr76WdShlMWH8JLos3RmALkt3ZsL4SRlHVDlN\n7b0rlIdjUxn+q3MfUkdJ90n6MO0E9pbUSdIzkj5Ofxb95159kvBQSb8HWkvaGfg78GixOyyWpOuA\nVYFHJJ0h6SZJb0p6W9I+6TbdJb0k6a300Sddvp2k5yXdCYxq6NhrMn7iJKZMnQbArNmzGTrsDXp0\n755tUGXywrOvsE+/3QDYp99uPP/MyxlHVF5N+b3L47FJpT3q4QrgyYhYC+gJfACcDgyJiB7AkPR1\nUepzxdzpwGEkyeso4AnghmJ3WKyIOFrSbsD2wEnAcxFxqKSOwBuSniU5abhzRMyW1AO4C9gkbWIz\nYL2I+HdDx16TcePHc+yZ5zB/wQIWLFjAvrvsxK7bbp11WIvsz1eexaa9N6TjUh14dth9XHP5zdx4\n7R1ceu0f2a//nnw1dhwn/+osADp37cQ9jw6ibbu2LFiwgF8cegD77PRLZkyfmfFRLJqm8t7VpCkf\nWzEkLQlsAxwCkFYFvks7ftulmw0GXgBOK2ofEXVXFiQtDqxFUob4qLA80ZAkfUaSVJ8EWgHz0lWd\ngF2BscDVQC9gPrBGRLSRtB1wdkRsX0vbRwJHAqzQbdmNRz7Z4J39BrHtXidlHUJFDX30sqxDsCLs\ncNAveee99xd5qMLKnVaI03c9tqR9H3P37z8HxhcsGhQRgwAk9QIGAe+T9IJHACcAX0ZEx+pfkDQp\nIooqSdTZE5a0J3Ad8AnJhRqrSDoqIv5RzA7LREC/iPjoBwulc4BxJP9YLYDZBatn1NZg+o8+CKDX\nuutkVfM2s0WhstxteXxEbLKQdS2BjYBfR8Trkq6ghNJDTepTE/4/YPuI2C4itiUpB1xeziCK8BTw\na6WXykjaMF3eAfgqIhYAvwAa3Uk4MysvpfeZK/ZRhzHAmIh4PX19H0lSHiepW7r/biSl0KLUJwl/\nExGFY1Q+LWWHZXIesBjwrqTR6WuAa4EBkoYBa1BH79fMrDYR8TXwH0lrpot2JClNPAIMSJcNAB4u\ndh+1zR2xf/r0PUlPAPeS1IR/ArxZ7A5LERHdC17+zwXt6Rjmwjktfpcuf4GkcG5mtqh+DdyRnhv7\nFBhI0oG9V9JhwBckebEotdWE9yp4Pg7YNn3+LdD0LoEys9wRlZ87IiLe4b+jrArtWI72F5qEI2Jg\nOXZgZlZJTXkWNQAktSIZJ7wuybAwACLi0ArGZWZWL83hzhq3AcuSjMMdSjKBz7RKBmVm1lzUJwmv\nHhF/AGZExGBgT6DxTi5qZpYj9blseW76c7Kk9YCvge4Vi8jMrL7UDGrCwKB0hqA/kIyNawecVdGo\nzMzqoSFGR1RanUk4Iqon6xlKMouZmZmVSW0Xa9Q600tEeKYUM7MS1dYTbt9gUZiZFaVx3CeuFLVd\nrPHHhgzEzKwYzeHEnJlZo9QUTszVZ5ywmZlViJOwmVmGPDrCzPKriV+sUT06Yk1gU5ILNSCZ4vLF\nSgZlZlZfOc/BdY+OkPQ0sFFETEtfn0Ny23szMytRfWrCKwGFd1f+Ds8dYWZWFvUZonYb8IakB0lu\nb7QfcGtFozIzq6cme7FGtYg4X9I/gK3TRQMj4u3KhmVmVrfmNE64DTA1Iq4AxkhapYIxmZk1G/W5\nvdHZJDe5WxO4meRW87cDW1Y2NDOzujXlIWrV9gM2BN4CiIixkpr25D4RxLz5WUdREY/+5ZSsQ6io\n5/76ctYhVMz2R/TOOoTKicg6gszUJwl/FxEhKQAkta1wTGZm9aP814Trk4TvlXQ90FHSEcChwA11\n/I6ZWQNQ0y9HRMSlknYGppLUhc+KiGcqHpmZWTNQnxNzf46I04BnalhmZmYlqM8QtZ1rWLZ7uQMx\nMyuGVNoja7XNovYr4BhgNUnvFqxqD7xa6cDMzOoimvYVc3cC/wAuBE4vWD4tIiZWNCozs2aitlnU\npgBTJF0BTCyYRa29pM0j4vWGCtLMbGFy3hGuV034r8D0gtcz0mVmZlai+owTVsR/L2eJiAWSfINQ\nM8teE7izRn16wp9KOl7SYunjBODTSgdmZtYc1CcJHw30Ab4ExgCbA0dWMigzs/pqskPUqkXEN8CB\nDRCLmVmzU9s44VMj4mJJV5HcUeMHIuL4ikZmZlaHZFL3RtCdLUFtPeEP0p/DGyIQM7Ni5DwH1zpO\n+NH05+CGC8fMrHmprRzxKDWUIapFxN4VicjMbBE05XLEpenP/YFlSW5pBHAQ8FkFYzIzazZqK0cM\nBZB0XkRsU7DqUUkvVjwyM7O6NJJhZqWoz5VvXSWtGhGfAqR3Wu5a2bCahw377ku7Nm2pqmpBVVUV\nQ26/JeuQSnLq1Vfw3PDhdO7QgaeuuBqA9//9KWdcdy1z5s6lZVUV5x55NL16rJFxpMXZ/U+HMm/2\nd8SCYMGCBTx30V0sv1EP1tlzC5ZcthPP/fkuJn3xTdZhlqypfS4bu/ok4d8AL0iqvkquO3BUxSJq\nZh66/ho6L9Ux6zDKot/2O/LL3fty8pWXf7/swltv4YT+B7HdRhvz/IjhXHTrLdx93gUZRlmaoZff\nx3czZn//eurY8bw26DE2/tmOGUZVfvn5XDaP2xs9KakHsFa66MOImFPZsCyPNl93PcZ8M+4HyyQx\nfeZMAKbNnMEynTplEVrFTPt6UtYhNGvJOOGsoyhNfW5v1AY4CVg5Io6Q1EPSmhHxWOXDa9okccCx\nxyOJAf32Y8D++2YdUtmddejhDDj3bC4YfDMLYgH3XXBx1iEVL4Ktj98fCD59aRT/fnl01hFVRHP4\nXDYm9SlH3AyMAHqnr8cAfwcaTRKW9GpE9Mk6jkX1+E2D6Na1K99OnMgBxxxPj+4r02ejDbMOq6xu\nf/IfnDnwcHbv3YfHXnmZ06+9itvPOS/rsIry/KX3MnvKDJZo35qtj9+faV9PYvy/vsw6rLLL2+cy\n73fWqM8EPqtFxMXAXICImEXyV0CjkccEDNCta3J+s2unTuyx/ba8Nfr9jCMqvwdeeI7dtki+v/fs\nsyUjP/5nxhEVb/aUGQDMmTaLse98Qqfuy2QcUWU0h89lY1KfJPydpNakF25IWg1oVDVhSdOVuETS\naEmjJPVP190maZ+Cbe+QlPmFJjNmzWLajBnfP39h2BusvfqqGUdVfksv1YnX30v+bH911Lt077Zc\nxhEVp2rxlrRcYrHvny+z9kpMGTsh46jKL4+fyyY/ixpwNvAksKKkO4AtgUMqGVSR9gd6AT2BLsCb\n6XjmG0hGeDwsqQPJtJwDMosy9e2EiQz47WkAzJs/n3677cKOfXrX8VuN2/GXXcKw0aOZNG0qvQ8f\nyIkHHsSFxxzHuTf+jXnz57PE4otzwa+OzTrMorRasg29j9oLALVowX/e/JBx73/Ocj1Xo1f/7Vii\nXWu2PHYfJo8Zz8tXPZhxtMVrip/Lxq7WJKxk7MeHJAluC5IyxAkRMb4BYltUWwF3RcR8YJykocCm\nEfGIpGskLU1yHPdHxLwf/7KkI0nnSV5h2WUrHmz3FZZn6N23171hjlx50ik1Ln/00strXJ4nM8ZP\n5dnz7/if5WNHfsLYkZ9kEFFl5O5z2QTurFFrEo6IkPRQRGwMPN5AMRWrtnfiNuBgknmRD61pg4gY\nBAwC6LXO2gudM8PMrJzqUxMeJmnTikdSuheB/pKqJHUFtgHeSNfdApwIEBHvZROemZVb9Tjhpl4T\n3h44WtJnJHdaFkkneYNKBraIAniQZBjdyPT1qRHxNUBEjJP0AfBQdiGaWSWoRSPIpCWoTxLeveJR\nlEBSZ2BiekfoU9LHj7dpA/QA7mrg8MzMalXbfMKtSG7yuTowCrixphNaWZK0HPAC/512s6ZtdgJu\nAi6LiCkNFJqZWb3U1hMeTHKBxkskveF1gBMaIqj6ioixQK1TckXEs8BKDRORmTW0xlDXLUVtSXid\niFgfQNKN/Pckl5lZ49AEhqjVNjpibvWTxlaGMDNrKmrrCfeUNDV9LqB1+rp6dMSSFY/OzKwODdER\nllRFcuf5LyOib3pzi7uBTsBbwC8i4rti2l5oTzgiqiJiyfTRPiJaFjx3Ajaz5uQE4IOC138GLo+I\nHsAk4LBiG67PxRpmZo2S0jtrlPKocx/SCsCeJPPQVE/nsANwX7rJYKDoSZfrM07YzKzRKkM5oouk\n4QWvB6XTGFT7C3Aq0D593RmYXHCubAywfLE7dxI2s+ZufERsUtMKSX2BbyJihKTtqhfXsGnR8804\nCZuZLdyWwN6S9gBaAUuS9Iw7SmqZ9oZXAMYWuwPXhM0s3yo4g09E/C4iVoiI7iSzMD4XEQcDzwMH\npJsNAB4uNnwnYTPLr/RijUqemFuI04CTJP2LpEZ8Y7ENuRxhZlYPEfECyVw1RMSnwGblaNc9YTOz\nDLknbGa5lvOpI5yEzSzfmsOk7mZmjVL17Y3yzDVhM7MMOQmbmWXI5Qgzy68mMKm7k7CZ5VrOc7DL\nEWZmWXJPeCHUoml+P82bPbfujXJs20M2zTqEihn14MisQ6iYWZNnZh1CZpyEzSzHSpr/oVFwEjaz\nXMt5DnZN2MwsS07CZmYZcjnCzHIruWw53/UIJ2Ezyy+R+7/nnYTNLNfy3hPO+XeImVm+OQmbmWXI\n5Qgzy7WcVyOchM0s31wTNjOzojkJm5llyOUIM8svuSZsZpah/GdhJ2Ezyy2R/1veuyZsZpYhJ2Ez\nswy5HGFmuZbzkrCTsJnlmG95b6WaP38+Ox48gG5Ld+WuKy/POpyifTVhPKf89Uq+nTyZFhL9d9iZ\nQ3bvy0V3DOb5t4azWMuWrLTMslx01HEs2bZt1uGW5F+ff8ERZ571/evPvxzLaUcezlEH/jTDqEo3\nf8ECjr7xArq078iFBx7Hg28+z31vDGHspG956KT/o0ObdlmH2CQ5CWfs+jvvZo1VujNtxoysQylJ\nVYsqfnfwIay7yqpMnzWL/c44hS3X78mW6/fktwf+nJZVVVx8121c98gDnHrQL7IOtySrr7wSz992\nC5B8iW6w137sse022QZVBveBL6vyAAAOcUlEQVS/MYSVuizLzDmzAVhvxdXo3WN9Trztsowja9p8\nYi5DX44bx9Mvv8LP99sn61BKtvRSS7HuKqsC0K51a1ZbfgXGTZrI1hv0omVVFQC9Vl+DrydMyDLM\nsntx+Ai6L788K3ZbNutQSvLt1EkM+9co9uy11ffLeiy7Est27JJhVPUjlfbImpNwhs645HLOOeHX\ntGjRtN6GMd9+w/uf/Zueq/X4wfL7XhjCtr02zCiqynjomWfZf5edsg6jZFc/fS9H7diPFo0hKy2q\nnGfhRvd/v6TukkZnHUelPfXiS3TptBS91lk761DKasbsWRx3+SWc8YuBtG/T5vvl1z50Hy2rqth7\ny/z/2V7tu7lzeeqlV9hrh+2zDqUkr338Lh3btmfNbitnHUqz5JpwRl5/512eHPoSz778KnO+m8O0\nGTM46oyzuP78c7MOrWhz583juMsvYe8tt2bXzbb4fvkDLz7P82+N4NYzzsn9mexCQ14bxvprrsHS\nnTtlHUpJRv/nE17950he/9dovps3l5lzZnH+Qzdyxr6HZR1aveT9irmKJWFJbYF7gRWAKuA8YE1g\nL6A18CpwVESEpI2Bm4CZwMsFbRwC7A20AVYDHoyIU9N1uwB/BJYAPgEGRsR0SRelvzMPeDoifivp\nJ8DZwHxgSkRk3h076/hjOev4YwF4efgIrrn19lwn4Ijg94OuZbXlV+DQPff+fvmLI99m0KMPcccf\nzqX1EktkGGH5Pfh00yhFHLHDfhyxw34AvPPZR9wz7JncJOCmoJI94d2AsRGxJ4CkDsAzEXFu+vo2\noC/wKHAz8OuIGCrpkh+10wvYEJgDfCTpKmAWcCawU0TMkHQacJKkq4H9gLXS5N4xbeMsYNeI+LJg\n2Q9IOhI4EmCFnJ9kycKIjz7koZeHsuaKK7HX704G4OSf/ozzbr2J7+bO5ZALky+YXquvwXmHHZVl\nqGUxc/Zshr7xJpeefkrWoVTM/W88x92vPcXE6VM5bNC5bL76epzS95dZh/UDjaSsW5JKJuFRwKWS\n/gw8FhEvSeon6VSSnm0n4D1JLwIdI2Jo+nu3AbsXtDMkIqYASHofWBnoCKwDvJL+ebs48BowFZgN\n3CDpceCxtI1XgFsk3Qs8UFOwETEIGATQa521oxz/APW11SYbs9UmGzfkLstuk7XW5uM77/+f5dtt\nmO/jWpg2rVrx0dNPZB1G2fXqvia9uq8JQL/NdqDfZjtkHFFd8p+FK5aEI+KfaZlhD+BCSU8DxwKb\nRMR/JJ0DtCKZCKm2pDen4Pl8kphF0qs+6McbS9oM2BE4EDgO2CEijpa0ObAn8I6kXhHRtMZKmVku\nVWx0hKTlgJkRcTtwKbBRumq8pHbAAQARMRmYIql6gOLB9Wh+GLClpNXTfbWRtEbaboeIeAI4kaSU\ngaTVIuL1iDgLGA+sWJ6jNDMrTSXLEesDl0haAMwFfgXsS1Km+Ax4s2DbgcBNkmYCT9XVcER8m560\nu0tS9dmeM4FpwMOSqnvYv0nXXSKpR7psCDCytEMzs8Yi59WIipYjnuJ/E+pwkmT5421HAD0LFp2T\nLr8FuKVgu74Fz58DNq1h15vV0P7+9Q7czHIl70PUGt3FGmZmzYkv1jCz/GoCU1m6J2xmliH3hM0s\n3/LdEXZP2MwsS+4Jm1muuSZsZmZFc0/YzHJL5L8n7CRsZvklcv/3fM7DNzPLN/eEzSzHlPtyhHvC\nZmYZck/YzHLNPWEzMyuae8Jmlm/57gi7J2xmtjCSVpT0vKQPJL0n6YR0eSdJz0j6OP25VLH7cBI2\ns/xSMql7KY86zANOjoi1gS2AYyWtA5xOchPiHiR36zm92ENwEjYzW4iI+Coi3kqfTwM+AJYH9gEG\np5sNJrl1W1FcEzazfCt9dEQXScMLXg+KiEH/uxt1BzYEXgeWiYivIEnUkpYududOwmaWa2UYoTY+\nIjapfR9qB9wPnBgRU8s5LM7lCDOzWkhajCQB3xERD6SLx0nqlq7vBnxTbPtOwmaWW9WzqJXyqLX9\nZIMbgQ8i4rKCVY8AA9LnA4CHiz0GlyNqMPKDD8d33nCzzxtwl12A8Q24v4bkY8uvhjy+lRtoP4tq\nS+AXwChJ76TLfg9cBNwr6TDgC+Anxe7ASbgGEdG1IfcnaXhdNam88rHlVy6OT4K6h5kVLSJeZuGX\ng+xYjn24HGFmliH3hM0s1zyBj5XD/4xJbEJ8bPnV1I+vUXBPuBGoaWB4U+Fjy6/cHF++O8JOwmaW\nby5HWLMl6fh0dqk7so6l0iS9mnUMlSKpu6TRWcfRXLknnFPpIHJFxIIMwzgG2D0i/l1sA5KqImJ+\nGWOqiIjok3UMVoN0FrU8c0+4zCQ9JGlEOvfokemy6ZLOlzRS0jBJy6TLV0tfvynpXEnTC9o5JV3+\nrqQ/psu6pz3Pa4G3gBWzOMY0luuAVYFHJJ0h6aY03rcl7VMQ70uS3koffdLl26VztN4JjMrqGBZF\n+h5K0iWSRksaJal/uu626mNOX98hae8MYmwr6fH0czZaUn9JZ6Xvy2hJg9IvbyRtnG73GnBsQRuH\nSHpA0pPpXLkXF6zbRdJr6Xv593Q+BSRdJOn99LN6abrsJ+k+R0p6sYH/KXLFSbj8Do2IjYFNgOMl\ndQbaAsMioifwInBEuu0VwBURsSkwtroBSbsAPYDNgF7AxpK2SVevCdwaERtGRENe1fcDEXE0Sczb\nkxzfc+lxbA9cIqktyfX0O0fERkB/4MqCJjYDzoiIdRo28pLsT/J+9AR2IjnObsANwEAASR2APsAT\nGcS3GzA2InpGxHrAk8DVEbFp+ro10Dfd9mbg+IjoXUM7vUjer/WB/komNu8CnAnslL6fw4GTJHUC\n9gPWjYgNgD+lbZwF7Jp+5iv7hSSV9siYk3D5HS9pJDCMpKfaA/gOeCxdPwLonj7vDfw9fX5nQRu7\npI+3SXq8a6XtAHweEcMqFXyRdgFOTy/rfAFoBawELAb8TdIokuMsTLhvlFLGyMhWwF0RMT8ixgFD\ngU0jYiiwupLpDA8C7o+IeRnENwrYSdKfJW0dEVOA7SW9nr4HOwDrpl8UHdO4AW77UTtDImJKRMwG\n3ie5pHgLkvfvlfR9HpAunwrMBm6QtD8wM23jFeAWSUcAVRU7YkqbN6IxnNRzTbiMJG1H0kPqHREz\nJb1AkpDmRkSkm82n7n93ARdGxPU/ar87MKOMIZeLgH4R8dEPFkrnAONIeo4tSP5nrdYYj6Mutf0f\nextwMHAgcGjDhPNDEfFPSRsDewAXSnqapNSwSUT8J30/WpEcRyy8JeYUPK/+vAp4JiIO+vHGkjYj\nuYT3QOA4YIeIOFrS5sCewDuSekXEhJIPsglyT7i8OgCT0gS8FknvoTbDgH7p8wMLlj8FHFpQc1te\nJUwa3QCeAn5dUG/cMF3eAfgqPXn4CyraI2oQL5L8eV4lqSuwDfBGuu4W4ESAiHgvi+AkLQfMjIjb\ngUuBjdJV49PP0gFpfJOBKZK2StcfXI/mhwFbSlo93VcbSWuk7XaIiCdIjr9Xun61iHg9Is4imQQo\ns/MXjZ17wuX1JHC0pHeBj0g+uLU5Ebhd0snA48AUgIh4WtLawGtpXpsO/JykV9IYnQf8BXg3TcSf\nkdQerwXul/QT4Hny2futFsCDJCWkkenrUyPia4CIGCfpA+Ch7EJkfZI69QJgLvArktvujCJ5T94s\n2HYgcJOkmSRforWKiG8lHQLcJWmJdPGZwDTgYUnVPezfpOsukdQjXTaE5N+sMrKvKJRE//0r2Rqa\npDbArIgISQcCB0XEPnX9njWs9OTqWxGx0OkW0/dyFLBRWou1BtBzjTXiiauuKqmNFXbbbUSWs8W5\nJ5ytjYGr097jZDKqJdrCpX/iv0Dy5/3CttkJuAm4zAnYFpWTcIYi4iWSk1bWSEXEWGCNOrZ5lmQ0\niNkicxI2s3xrBMPMSuEkbGa51hjG+pbCSdjM8ktU9PZGDcHjhK3BSOos6Z308bWkLwteL17G/ewk\nqdahYpIOl/SXRWx3jKSOpUVn9kPuCVuDSa+Yqh7Mfw4wPSJ+MOogHSmS9exwZg3GPWHLnKTV0xm3\nriOdHU7S5IL1B0q6IX2+jJJZvoZLekNSrVclStpCycxfb0t6Jb2AoNrKkp6S9JGkMwt+Z0Da9juS\nrpXk/08aLc8dYVYu6wAD0zkHavtcXglcHBHD0rk0HgPWq2X7D4CtImK+pN1IZvnqn67bLP3d74A3\nJT0GzCOZFaxPRMyTNIjkkvI7/7dpaxSyz6MlcRK2xuKTiHiz7s3YCVizoAezlKTWETFrIdt3BG6V\ntFoN656KiEmQzANNMktaS2BTYHi6j9bAf+p/GGaLxknYGovCeSUW8MP+TauC5wI2i4jv6tnu+STJ\n9tp08pknC9b9+Jr9SNu/KSL+UM/2zUriWpc1OulJuUmSeqT12P0KVj/LD+8E0auO5joAX6bPD/nR\nul0kdUznfdiHZA7cZ4GfKpnEvHpEh6+Ga8TyXhN2ErbG6jSSXusQYEzB8mNJplR8V9L7/PcuJQvz\nZ5IZvV6pYd3LJLXet0kma38nIkYBfwSeTWfDexpYprRDsYqpHidcyiPrQ/AsamaWVz3XWjOeGnRd\nSW1023aHTGdRc0/YzCxDPjFnZrnWGOq6pXASNrN8cxI2M8uGaBwjHErhmrCZWYachM3MMuRyhJnl\nVxOYT9hJ2MxyzTVhMzMrmpOwmVmGXI4ws3zLeTnCSdjMck05PzHncoSZWYachM3MMuRyhJnll+Sa\nsJlZlvI+TthJ2MzyLedJ2DVhM7MMOQmbmWXI5Qgzy7W8jxN2Ejaz/BKuCZuZWfGchM3MMqSIyDoG\nM7OiSHoS6FJiM+MjYrdyxFMMJ2Ezswy5HGFmliEnYTOzDDkJm5llyEnYzCxDTsJmZhlyEjYzy5CT\nsJlZhpyEzcwy5CRsZpah/weG57LiHitkkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred_NB) \n",
    "my_tags = ['anger', 'fear', 'joy', 'sadness']\n",
    "plot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.78\n",
      "testing accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred_NB)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred_NB)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer here\n",
    "We can compare the accuracy and confusion matrix:\n",
    "For accuracy, NB's testing accuracy is higher than DT.\n",
    "From the confusion matrix, one of the obivious difference is that DT tends to categorize sadness to fear but NB will not.\n",
    "Comapring to the training accuracy, we can see that decision tree is much higher than NB. The reason may be that the decision tree model is too simple, which cause the overfitting. Although it's pros is easy to understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JackChuang\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (3613, 500)\n",
      "y_train.shape:  (3613,)\n",
      "X_test.shape:  (347, 500)\n",
      "y_test.shape:  (347,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'fear' 'joy' 'sadness']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 0    anger\n",
      "1    anger\n",
      "2    anger\n",
      "3    anger\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (3613,)\n",
      "y_test.shape:  (347,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (3613, 4)\n",
      "y_test.shape:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  500\n",
      "output_shape:  4\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32064     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 36,484\n",
      "Trainable params: 36,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3613 samples, validate on 347 samples\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(32, 500), b.shape=(500, 64), m=32, n=64, k=500\n\t [[{{node dense_1/MatMul}} = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/dense_1/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_input_1_0_0/_63, dense_1/kernel/read)]]\n\t [[{{node loss/mul/_85}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_520_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ef3615086d52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                     validation_data = (X_test, y_test))\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training finish'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(32, 500), b.shape=(500, 64), m=32, n=64, k=500\n\t [[{{node dense_1/MatMul}} = MatMul[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/dense_1/MatMul_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_input_1_0_0/_63, dense_1/kernel/read)]]\n\t [[{{node loss/mul/_85}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_520_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n",
    "<table><tr>\n",
    "    <td><img src=\"pics/pic3.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td><img src=\"pics/pic4.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "training_log.epoch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.title('Training Accuracy per epoch')\n",
    "plt.plot(training_log.epoch,training_log.acc,color = 'b',label='Train accuracy')\n",
    "plt.plot(training_log.epoch,training_log.val_acc,color = 'r',label='Val accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.subplot(122)\n",
    "plt.title('Training loss per epoch')\n",
    "plt.plot(training_log.epoch,training_log.loss,color = 'b',label = 'Train loss')\n",
    "plt.plot(training_log.epoch,training_log.val_loss,color = 'r',label = 'Val loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training accuracy is growing higher and the training losss is lowing. This shows that it's performance is getting better on training data.\n",
    "However, the val accuracy once reach the peak and start decreasing, same to the val loss. This shows that the model is getting overfitting. If we want to get the most proper model, we should stop at the highest val accuracy and the lowest val loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 7 (Take home): **  \n",
    "\n",
    "Now, we have the word vectors, but our input data is a sequence of words (or say sentence). \n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer here\n",
    "There are a vary of method to use word vectors to represent the sentence. The following list two common methods:\n",
    "1. Average: Since we use same method to transfer word to vector. The dimension of all words are the same. So we can use average to combine them together. If the dimension is different, we can use padding to make them into same size.\n",
    "2. Concatenate: We can simplely cancatenate all the word vector together. The pros is that it contain more information than average. The cons is that due to the extreme large dimension, it may cause curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 8 (Take home): **  \n",
    "\n",
    "Generate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "word_list = ['happy', 'angry', 'sad', 'fear']\n",
    "\n",
    "topn = 15\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \n",
    "mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('data_words: ', data_words)\n",
    "print('mining_words: ', mining_words)\n",
    "\n",
    "target_words = happy_words + angry_words + data_words + mining_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.vocab.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
